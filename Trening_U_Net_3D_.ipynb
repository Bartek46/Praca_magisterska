{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# Proces szkolenia modeli\n","\n","# Zastrzeżenie\n","# Nie zalecam przeprowadzać pełnego procesu szkolenia na własnym sprzęcie, trening modelu ma bardzo duże wymagania sprzętowe\n","# Możliwe jest jednak przeprowadzenie poglądowego procesu szkolenia na słabszym sprzęcie, ale dopiero po zmianie zmiennej\n","#   w klasie Config z 'self.set_size = [90, 15, 20]' na 'self.set_size = [2, 2, 2]'\n","# Biblioteka TensorFlow wykorzystuje cały dostepny RAM, nawet jeżeli uczy się na bardzo małym zbiorze\n","# Dla 2 skanów jedna epoka uczenia trwa ok. 3-4 minuty"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-03T16:18:48.362755Z","iopub.status.busy":"2024-09-03T16:18:48.361972Z","iopub.status.idle":"2024-09-03T16:19:02.135396Z","shell.execute_reply":"2024-09-03T16:19:02.134390Z","shell.execute_reply.started":"2024-09-03T16:18:48.362717Z"},"trusted":true},"outputs":[],"source":["import os\n","import gc\n","import io\n","import math\n","import gzip\n","from datetime import datetime, timedelta\n","import numpy as np\n","import nibabel as nib\n","from tensorflow import data, cast, squeeze, math, reduce_mean\n","import tensorflow.keras.backend as K\n","from tensorflow.keras import Model, Input\n","from tensorflow.keras.layers import Conv3D, Conv3DTranspose, MaxPooling3D, concatenate, BatchNormalization, Activation, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, CSVLogger, LearningRateScheduler, Callback\n","from tensorflow.keras.losses import BinaryCrossentropy\n","\n","np.set_printoptions(suppress=True, precision=8)\n","# Ustaw poziom logowania na ERROR\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = INFO, 1 = WARNING, 2 = ERROR, 3 = FATAL\n","os.environ['XLA_FLAGS'] = '--xla_cpu_fast_math_honor_infs=false --xla_cpu_fast_math_honor_nans=false --xla_hlo_profile=false'"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-03T16:21:01.490056Z","iopub.status.busy":"2024-09-03T16:21:01.489404Z","iopub.status.idle":"2024-09-03T16:21:01.541032Z","shell.execute_reply":"2024-09-03T16:21:01.540029Z","shell.execute_reply.started":"2024-09-03T16:21:01.490019Z"},"trusted":true},"outputs":[],"source":["class Config():\n","    \"\"\"\n","    Zbiór zmiennych globalnych i podlegająych zmianom w trakie procesu szkolenia.\n","    \"\"\"\n","\n","    def __init__(self, ax=0, env='kaggle'):\n","        # Oś podziału danych - 0, 1, 2\n","        self.ax = ax\n","        # Długość treningu\n","        self.train_epochs = 1\n","        # Jeżeli doszkalam model to muszę wpsiać na ilu epok model jest już przeszkolony\n","        self.previous_epochs = 0 \n","        # Liczba części na którą dzieli skan\n","        self.number_of_subset = [5, 5, 6][self.ax]\n","        # Liczba danych biorących udział w procesie szkolenia\n","        # self.set_size = [90, 15, 20] # train, val, test - max 90, 15, 20\n","        # Poglądowe szkolenie\n","        self.set_size = [2, 2, 2]\n","        # Liczba filtrów w pierwszej warstwie \n","        self.filter_size_start = 32\n","        # Nazwa modelu\n","        self.name_output = f\"K_{self.ax}_{str(self.filter_size_start)}_NFBS-20_{str(int([48, 48, 32][self.ax]))}_{str('UO-')}{self.previous_epochs + self.train_epochs}.weights.h5\"\n","        \n","            # Nie podlega zmianom\n","        # Wybór środowiska\n","        self.env = env\n","        self.lr = 1e-5\n","        self.seed = 123\n","        self.batch_size = 1\n","        # Rozmiar jednej części\n","        self.img_size = [[48, 256, 192], [256, 48, 192], [256, 256, 32]][self.ax]\n","        # Liczba filtrów w sieci neuronowej, [32, 64, 128, 256, 512]\n","        self.filter_size = [self.filter_size_start * 2**x for x in range(5)]\n","        # Liczba iteracji w epoce\n","        self.subset_size = [self.number_of_subset * x for x in self.set_size]\n","        # Poziom informacji zwrotnych podczas treningu, kolejno dla Early, Reduce, Checkpoint, Fit\n","        # 0 - brak jakichkolwiek informacji w konsoli\n","        # 1 - w konsoli będzie wyświetlany pasek postępu dla każdej epoki\n","        # 2 - w konsoli będzie wyświetlany pasek postępu dla każdego batcha w epoce\n","        self.verbose = [1, 1, 1, 1]\n","        # Określa typ wczytywanych danych, standardowo float64 \n","        self.dtype = np.float32\n","\n","        if self.env == 'kaggle':\n","            self.dataset_path = '/kaggle/input/nfbs-dataset-20/NFBS_Dataset_20'\n","            self.path_end = '.nii'\n","        elif self.env == 'laptop':\n","            self.dataset_path = 'NFBS_Dataset_20'\n","            self.path_end = '.nii.gz'\n","\n","\n","\n","class DataNFBS():\n","    \"\"\"\n","    Klasa wczytująca i przetwarzająca dane.\n","    Zwraca dane w formacie 'dataset' biblioteki TensorFlow, gotowe do uczenia.\n","\n","    Metody\n","    ------\n","    subset_scope\n","        Zwraca zakresy podziału skanu na częsci\n","    normalization\n","        Wykonuje normalizację\n","    extract_gz\n","        Rozpakowanie pliku\n","    read_nii_file\n","        Przekształca dane z formatu .nii do listy\n","    read_dataset\n","        Zapisuje dane w formacie 'dataset'\n","    read_data\n","        Przekierowuje wczytanie wybranych podzbiorów\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        self.config = config\n","        self.path = ['train', 'val', 'test']\n","\n","\n","    def subset_scope(self, ax):\n","        if ax == 2:\n","            ranges = [(8, 40), (37, 69), (66, 98), (95, 127), (124, 156), (153, 185)]\n","        elif ax == 1:\n","            ranges = [(0, 48), (45, 93), (90, 138), (135, 183), (180, 228)]\n","        elif ax == 0:\n","            ranges = [(28, 76), (73, 121), (118, 166), (163, 211), (208, 256)]\n","\n","        return ranges\n","\n","\n","    def normalization(self, data):\n","        # Wszystko powyżej 600 na 600\n","        maxx = 600\n","        data = np.where(data > maxx, maxx, data)\n","        # Dzielimy przez 600, sprowadzamy do zakresu [0, 1]\n","        data = data / maxx\n","\n","        return data\n","    \n","\n","    def extract_gz(self, file_path):\n","        # Rozpakowanie pliku .gz\n","        with gzip.open(file_path, 'rb') as f_in:\n","            file_content = f_in.read()\n","        \n","        # Wczytanie danych z rozpakowanego strumienia\n","        file_stream = io.BytesIO(file_content)\n","        img = nib.Nifti1Image.from_bytes(file_stream.getvalue())   \n","\n","        return img\n","\n","\n","    def read_nii_file(self, file_path, data):\n","        # jeśli wczytujemy plik na laptopie to musimy go najpierw rozpakować a dopiero potem wczytać\n","        if self.config.env == 'laptop':\n","            img = self.extract_gz(file_path)\n","        # jeśli wczytujemy plik z kaggle to jest on już rozpakowany, od razu możemy wczytać\n","        elif self.config.env == 'kaggle':\n","            img = nib.load(file_path)\n","\n","        # img to wewnętrzny typ nibabel, wyciągamy dane i konwertujemy do określonego typu\n","        # scan to czyste dane [256, 256, 192]\n","        scan = img.dataobj[:, :, :].astype(self.config.dtype)\n","\n","        # normalizacja, jeśli to nie jest maska\n","        if 'mask' not in file_path:\n","            scan = self.normalization(scan)\n","\n","        # zapisujemy po kawałkach\n","        ranges = self.subset_scope(self.config.ax)\n","        for a, b in ranges:\n","            if self.config.ax == 2:\n","                data.append(scan[:, :, a:b])\n","            elif self.config.ax == 1:\n","                data.append(scan[:, a:b, :])\n","            elif self.config.ax == 0:\n","                data.append(scan[a:b, :, :])\n","\n","        return data\n","\n","\n","    def read_dataset(self, pos):\n","        # position zawiera nazwę pozbioru\n","        position = self.path.index(pos)\n","        X_data = []\n","        y_data = [] # maska\n","\n","        # pobieramy wszystkie nazwy plików z folderu podzbioru\n","        files = os.listdir(self.config.dataset_path + '/' + self.path[position])\n","        # sortujemy pliki po indeksie, który jest na pozycji 5:8\n","        files_sort = sorted(set([x[5:8] for x in files]))\n","\n","        # wczytujemy X pierwszych plików\n","        for i in range(self.config.set_size[position]):\n","            X_data = self.read_nii_file(f\"{self.config.dataset_path}/{self.path[position]}/NFBS_{str(files_sort[i])}_{self.path[position]}{self.config.path_end}\", X_data)\n","            y_data = self.read_nii_file(f\"{self.config.dataset_path}/{self.path[position]}/NFBS_{str(files_sort[i])}_mask_{self.path[position]}{self.config.path_end}\", y_data)\n","\n","        # zmieniam listę w której są 3 wymiarowe dane na 4 wymiarowe dane \n","        X_data = np.stack(X_data, axis=0)\n","        y_data = np.stack(y_data, axis=0)\n","        print(X_data.shape)\n","        print(y_data.shape)\n","\n","        # tworzę dataset\n","        # prefetch przygotowuje kolejne partie danych w tle, dzięki temu model nie musi czekać na załadowanie kolejnych danych w trakcie treningu\n","        # repeat powiela dane, umozliwiając trenowanie na więcej niż jednej eopoce\n","        dataset = data.Dataset.from_tensor_slices((X_data, y_data)).batch(self.config.batch_size).prefetch(data.AUTOTUNE).repeat()\n","        del X_data, y_data\n","        gc.collect()\n","\n","        return dataset\n","\n","\n","    def read_data(self, status='learning'):\n","        # Wczytuje wszystkie podzbiory\n","        if status == 'all':\n","            train_dataset = self.read_dataset('train')\n","            val_dataset = self.read_dataset('val')\n","            test_dataset = self.read_dataset('test')\n","            return train_dataset, val_dataset, test_dataset\n","        # Wczytuje podzbiory potrzebne do treningu\n","        elif status == 'learning':\n","            train_dataset = self.read_dataset('train')\n","            val_dataset = self.read_dataset('val')\n","            return train_dataset, val_dataset, None\n","        # Wczytuje tylko podzbiór testowy, do oceny modelu\n","        elif status == 'test':\n","            test_dataset = self.read_dataset('test')\n","            return None, None, test_dataset\n","        else:\n","            print(\"Popraw.\")\n","\n","\n","\n","class UNetModel3D():\n","    \"\"\"\n","    Architektura modelu\n","    \"\"\"\n","    \n","    def __init__(self, config):\n","        self.config = config\n","        self.activation = 'relu'\n","\n","    def conv_block(self, inp, filters):\n","        x = Conv3D(filters, (3, 3, 3), padding='same')(inp)\n","        x = BatchNormalization(axis=4)(x)\n","        x = Activation(self.activation)(x)\n","        x = Conv3D(filters, (3, 3, 3), padding='same')(x)\n","        x = BatchNormalization(axis=4)(x)\n","        x = Activation(self.activation)(x)\n","        return x\n","\n","    def encoder_block(self, inp, filters):\n","        x = self.conv_block(inp, filters)\n","        p = MaxPooling3D(pool_size=(2, 2, 2))(x)\n","        # x - to ta długa strzałka w bok\n","        return x, p\n","\n","    def decoder_block(self, inp, filters, concat):  # concat to od strzałki (x)\n","        x = Conv3DTranspose(filters, (3, 3, 3), strides=(2, 2, 2), padding='same')(inp)\n","        x = BatchNormalization(axis=4)(x)\n","        x = Activation(self.activation)(x)\n","        x = concatenate([x, concat])\n","        x = self.conv_block(x, filters)\n","        return x\n","\n","    def create_model(self):\n","        inputs = Input((self.config.img_size[0], self.config.img_size[1], self.config.img_size[2], 1)) # 1 bo tylko odcień szarości\n","\n","        x1, p1 = self.encoder_block(inputs, self.config.filter_size[0])\n","        x2, p2 = self.encoder_block(p1, self.config.filter_size[1])\n","        x3, p3 = self.encoder_block(p2, self.config.filter_size[2])\n","        x4, p4 = self.encoder_block(p3, self.config.filter_size[3])\n","\n","        mid = self.conv_block(p4, self.config.filter_size[4])\n","        mid = Dropout(0.2)(mid)\n","\n","        y1 = self.decoder_block(mid, self.config.filter_size[3], x4)\n","        y2 = self.decoder_block(y1, self.config.filter_size[2], x3)\n","        y3 = self.decoder_block(y2, self.config.filter_size[1], x2)\n","        y4 = self.decoder_block(y3, self.config.filter_size[0], x1)\n","\n","        outputs = Conv3D(1, (1, 1, 1), activation='sigmoid')(y4)\n","\n","        model = Model(inputs=inputs, outputs=outputs)\n","\n","        print('Całkowita liczba parametrów: {:,}'.format(model.count_params()))\n","\n","        return model\n","  \n","\n","\n","class ModelTrainer():\n","    \"\"\"\n","    Klasa przygotowująca i przeprowadzająca proces szkolenia\n","\n","    Metody\n","    ------\n","    linear_decay_lr\n","        Zmienia dynamicznie wartość stałej uczącej podczas treningu\n","    dice_coef\n","        Obliczanie metryki, która jest informacją zwrotną po każdej epoce\n","    LearningRateLogger\n","        Klasa - niestandardowy callback do zapisywania wartość stałej uczącej do logów\n","    train_model\n","        Przeprowadza proces szkolenia\n","    \"\"\"\n","\n","    def __init__(self, model, config):\n","        self.model = model\n","        self.config = config\n","\n","        \n","    def linear_decay_lr(self, epoch, lr):\n","        # Jeżeli doszkalamy model to uwzględniamy na ilu epokach został juz przeszkolony\n","        epoch_new = self.config.previous_epochs + epoch\n","\n","        if epoch_new < 2:\n","            return 1e-3\n","        elif epoch_new < 6:\n","            return 1e-4\n","        elif epoch_new < 10:\n","            return 8e-5\n","        elif epoch_new < 12:\n","            return 6e-5\n","        elif epoch_new < 16:\n","            return 4e-5\n","        elif epoch_new < 20:\n","            return 3e-5\n","        elif epoch_new < 50:\n","            initial_lr = 3e-5  \n","            final_lr = 1e-5    \n","            # Obliczenie nowej wartości learning rate na podstawie epoki\n","            new_lr = final_lr + (initial_lr - final_lr) * (1/30 * (50 - epoch_new))\n","            return new_lr\n","        elif epoch_new < 70:\n","            initial_lr = 1e-5\n","            final_lr = 4e-6\n","            new_lr = final_lr + (initial_lr - final_lr) * (1/20 * (70 - epoch_new))\n","            return new_lr\n","        else:\n","            return 4e-6\n","\n","    \n","    def dice_coef(self, y_true, y_pred):\n","        # y_true - oryginalna maska, wartości 0 i 1\n","        # y_pred - maska z predykcji, wartości od 0 do 1\n","\n","        # Dodajemy epsilon, żeby nie dzielić przez 0\n","        eps = K.epsilon()\n","        # greater sprawdza czy wartości są większe od 0.5, jeżeli tak to True, jeżeli nie to False\n","        # nastepnie wektor jest castowany, czyli wartości boolowskie są zmieniane na float32\n","        y_pred = cast(math.greater(y_pred, 0.5), 'float32')\n","        y_true = cast(y_true, 'float32')\n","        y_pred = squeeze(y_pred, axis=-1)\n","        # mnożenie żeby zostawić tylko te gdzie powielają się 1\n","        # axis=[1, 2, 3] oznacza po których osiach ma się sumować, 0 to wymiar przykładów uczących\n","        intersec = K.sum(y_true * y_pred, axis=[1, 2, 3])\n","        # liczenie współczynnika dice \n","        dice = (2 * intersec + eps) / (K.sum(y_true, axis=[1, 2, 3]) + K.sum(y_pred, axis=[1, 2, 3]) + eps)\n","        # dice to wektor ze współczynnikiem dla każdego przykładu uczącego\n","\n","        # Zwraca średnią dla całego podzbioru\n","        return reduce_mean(dice)\n","    \n","    \n","    # Niestandardowy callback do logowania learning rate\n","    class LearningRateLogger(Callback):\n","        def on_epoch_end(self, epoch, logs=None):\n","            logs = logs or {}\n","            logs['learning_rate'] = K.get_value(self.model.optimizer.learning_rate.numpy())\n","    \n","\n","    def train_model(self, train_dataset, val_dataset):\n","        # Wyświetl orientacyjny czas zakończenia szkolenia, jedna epoka trwa 504 sekundy na platformie Kaggle na kracie P100\n","        if self.config.env == 'kaggle':\n","            print(f'Start: {(datetime.now() + timedelta(hours=2)).strftime(\"%H:%M:%S\")}')\n","            print(f'Koniec: {(datetime.now() + timedelta(hours=2) + timedelta(seconds=(504 * self.config.train_epochs))).strftime(\"%H:%M:%S\")}')\n","        \n","        # optimizer do szybszego spadku gradientowego\n","        # funkcja straty jest używana do aktualizacji wag\n","        # metryka służy do monitorowania jakości modelu, informacyjnie dla nas\n","        self.model.compile(optimizer=Adam(learning_rate=self.config.lr), loss=BinaryCrossentropy(), metrics=[self.dice_coef])\n","\n","        # callbacki są używane do wykonywania określonych działań na końcu epok, podczas trenowania modelu\n","        callbacks = [\n","                # Monitoruje wartość metryki (domyślnie val_loss) i przerywa trenowanie, jeśli ta metryka przestaje się poprawiać.\n","                # patience=10 - trenowanie zostanie przerwane, jeśli przez 10 kolejnych epok nie nastąpi poprawa wartości monitorowanej metryki\n","                # verbose=1 - zobaczysz w konsoli komunikaty, kiedy EarlyStopping zdecyduje się przerwać trenowanie.\n","            EarlyStopping(patience=10, verbose=self.config.verbose[0]),\n","                # Dynamicznie zmniejsza learning rate w przypadku, gdy metryka monitorowana (domyślnie na val_loss) przestaje się poprawiać.\n","                # factor=0.5 - Mnożnik, przez który zostanie pomnożona bieżąca szybkość uczenia. W tym przypadku learning rate zostanie zmniejszony do 10% swojej wartości.\n","                # patience=5 - Liczba epok bez poprawy, po których szybkość uczenia zostanie zmniejszona.\n","                # min_lr=0.00001 - Minimalna wartość learning rate, poniżej której nie zostanie obniżona.\n","            # ReduceLROnPlateau(factor=0.8, patience=2, min_lr=1e-06, verbose=self.config.verbose[1]),\n","                # Zapisuje wagi modelu na dysku podczas trenowania.\n","                # save_best_only=True - Zapisuje tylko te wagi modelu, które osiągnęły najlepszą wartość monitorowanej metryki (domyślnie na val_loss).\n","                # save_weights_only=True - Zapisuje tylko wagi modelu, a nie całą jego architekturę. Przydatne, gdy architektura modelu nie zmienia się podczas trenowania.\n","            ModelCheckpoint(self.config.name_output, verbose=self.config.verbose[2], save_best_only=True, save_weights_only=True),\n","                # Zapisuje logi trenowania do pliku CSV.\n","            CSVLogger('training_log.csv', append=True),\n","               # Dynamicznie zmienia learning rate w oparciu o zdefiniowaną funkcję.\n","            LearningRateScheduler(self.linear_decay_lr),\n","            self.LearningRateLogger()\n","        ]\n","\n","        history = self.model.fit(\n","            train_dataset,\n","            # dane do walidacji, w specjalnym formacie\n","            validation_data = val_dataset,\n","            # liczba epok\n","            epochs=self.config.train_epochs,\n","            # Liczba określająca ile poditeracji na batchach ma się wykonać podczas epoki.\n","            # Ma sens w przypadku sekwencyjnym dostarczaniu danych (generator lub tf.data.Dataset) bo model nie wie ile dostanie danych i w którym momencie dostaje ponownie te same dane.\n","            # Metoda repeat na dataset w klasie DataNFBS będzie dostarczać dane w nieskończoność\n","            steps_per_epoch = int(self.config.subset_size[0] / self.config.batch_size),\n","            # To samo dla zbioru walidacyjnego.\n","            validation_steps = int(self.config.subset_size[1] / self.config.batch_size),\n","            # lista callbacków\n","            callbacks=callbacks,\n","            verbose=self.config.verbose[3]\n","        )\n","\n","        return history"]},{"cell_type":"markdown","metadata":{},"source":["___"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-09-03T16:21:03.128598Z","iopub.status.busy":"2024-09-03T16:21:03.127698Z","iopub.status.idle":"2024-09-03T16:21:03.132830Z","shell.execute_reply":"2024-09-03T16:21:03.131947Z","shell.execute_reply.started":"2024-09-03T16:21:03.128557Z"},"trusted":true},"outputs":[],"source":["# ax - oś wzdłuż której dzielony jest skan, płaszczyzna czołowa --> 0 --> model czołowy\n","# env - środowisko w którym przeprowadzamy proces szkolenia\n","    # laptop - na własnym sprzęcie\n","    # kaggle - platforma Kaggle, należy odpowiednio wczytać pliki\n","\n","config = Config(ax=0,\n","                env='laptop')"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T15:16:04.191321Z","iopub.status.busy":"2024-09-02T15:16:04.190535Z","iopub.status.idle":"2024-09-02T15:18:54.527917Z","shell.execute_reply":"2024-09-02T15:18:54.527095Z","shell.execute_reply.started":"2024-09-02T15:16:04.191273Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(10, 48, 256, 192)\n","(10, 48, 256, 192)\n","(10, 48, 256, 192)\n","(10, 48, 256, 192)\n"]}],"source":["# Wybór danych które chcemy wczytać\n","# 'all' - wczytają się wszystkie dane\n","# 'learning' - wczytają się dane potrzebne do treningu\n","# 'test' - wczyta się tylko zbiór testowy, do ocena modelu, wykorzystywane w innym notebooku\n","\n","train_dataset, val_dataset, test_dataset = DataNFBS(config).read_data('learning')\n","# learning wczytuje się 2-3 minuty na Kaggle"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-03T16:21:10.520035Z","iopub.status.busy":"2024-09-03T16:21:10.519669Z","iopub.status.idle":"2024-09-03T16:21:11.558779Z","shell.execute_reply":"2024-09-03T16:21:11.557838Z","shell.execute_reply.started":"2024-09-03T16:21:10.520000Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Całkowita liczba parametrów: 25,896,545\n"]}],"source":["# Implementacja modelu, losowe wagi\n","model = UNetModel3D(config).create_model()\n","# Przygotowanie procesu szkolenia\n","model_trainer = ModelTrainer(model, config)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T17:05:45.141956Z","iopub.status.busy":"2024-09-02T17:05:45.141641Z","iopub.status.idle":"2024-09-02T17:05:46.561044Z","shell.execute_reply":"2024-09-02T17:05:46.560281Z","shell.execute_reply.started":"2024-09-02T17:05:45.141924Z"},"trusted":true},"outputs":[],"source":["# Wczytanie wag jeżeli doszkalamy model\n","# model.load_weights('/kaggle/input/modele/K_0_32_NFBS-20_48_UO-24_O.weights.h5')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T17:05:49.837737Z","iopub.status.busy":"2024-09-02T17:05:49.837343Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18s/step - dice_coef: 0.3435 - loss: 0.5519 \n","Epoch 1: val_loss improved from inf to 0.91469, saving model to K_0_32_NFBS-20_48_UO-1.weights.h5\n","\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 21s/step - dice_coef: 0.3495 - loss: 0.5466 - val_dice_coef: 4.6025e-11 - val_loss: 0.9147 - learning_rate: 0.0010\n"]}],"source":["# Przeprowadzenie procesu szkolenia\n","history = model_trainer.train_model(train_dataset, val_dataset)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-09-02T14:17:27.564867Z","iopub.status.busy":"2024-09-02T14:17:27.564434Z","iopub.status.idle":"2024-09-02T14:17:28.203284Z","shell.execute_reply":"2024-09-02T14:17:28.202289Z","shell.execute_reply.started":"2024-09-02T14:17:27.564795Z"},"trusted":true},"outputs":[],"source":["# Zapisujemy ręcznie końcowy model, o ile to potrzebne\n","# model.save_weights('K_0_32_NFBS-20_48_UO-2444.weights.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":5142319,"sourceId":8595810,"sourceType":"datasetVersion"},{"datasetId":5622980,"sourceId":9288614,"sourceType":"datasetVersion"},{"datasetId":5143697,"sourceId":9302028,"sourceType":"datasetVersion"}],"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
